# Spark version: 3.1.2
# Haddop version: 3.3.1
# Aim: Perform operations on cpu log files and also do visualization of the result.
# Operations:
Display users and their record counts
Finding users with highest number of average hours
Finding users with lowest number of average hours
Finding users with highest numbers of idle hours
# Step1: start Hadoop
start-all.sh
# Step2: In python program fetch required library's use findspark for spark
# Step3: Create dataframe for query's
# Step4: Visualize the output of every operation 
